# -*- coding: utf-8 -*-
"""bp2_ML_fn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11x2EHNq5j6XWeXUDomT1A9nSAF4nKXb3
"""

from pymongo import MongoClient
import numpy as np
import pandas as pd 
import pymongo as pym
import os
import sys
import json
import string
import re
from typing import List

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer ##text to number
from sklearn.model_selection import train_test_split


from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

import warnings
warnings.filterwarnings("ignore")

import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
stop_words = set(stopwords.words('english'))

#!pip install pymongo
#!pip3 install pymongo[srv]
#!pip3 install pymongo[tls]
#!pip install dnspython

from pymongo import MongoClient
import pymongo as pym
import os
import sys
import json
from typing import List

client = MongoClient("mongodb+srv://yas:EGkaWh2eYmF2A3MU@twitter.khctm.mongodb.net/test?retryWrites=true&w=majority")

db = client["twitter"]

db.list_collection_names()

data = db.unclean_tweets

## converting the collections to pandas dataframe
df = pd.DataFrame(list(data.find()))
## Dropping columns
df = df.drop(columns = ['_id', '', 'Date', 'username', 'description', 'following', 'followers',
       'totaltweets', 'retweetcount', 'hashtags', 'location'])

df.info()

##preprocessing
def remove_characters(text, characters):
  r = re.findall(characters, text)
  for word in r:
    text = re.sub(word, "", text)
    text= re.sub(r"http\S+|www\S+|https\S+", "", text, flags=re.MULTILINE)
  return text

STOPWORDS = set(nltk.corpus.stopwords.words('english'))
STOPWORDS

##removing @
df['clean_text'] = np.vectorize(remove_characters)(df['text'], "@[\w]*")
df['clean_text'] = df['clean_text'].str.replace("[^a-zA-Z#]", " ")
df['clean_text'] =  df['clean_text'].apply(lambda x: " ".join([w for w in x.split() if len(w)>3]))

token_text = df['clean_text'].apply(lambda x: x.split())
token_text.head()

from nltk.stem.porter import PorterStemmer
stemmer = PorterStemmer()
token_text = token_text.apply(lambda x: [stemmer.stem(word) for word in x])
token_text.head()

for i in range (len(token_text)):
  token_text[i] = " ".join(token_text[i])
  
df['clean_text'] = token_text
df.head()

from nltk.tokenize import TweetTokenizer

tokenize = TweetTokenizer()

df['tokens'] = df['clean_text'].apply(tokenize.tokenize)

df[40:100][['text','clean_text', 'tokens']]

punctuation = list(string.punctuation)

def remove_punctuation(word_list):
     return [w for w in word_list if w not in punctuation]

df['tokens'] = df['tokens'].apply(remove_punctuation)

corpus = df['tokens'].sum()

from nltk import FreqDist

corpus_freq = FreqDist(corpus)

len(corpus_freq)

repeated = [w for w in corpus_freq.most_common() if w[1] >= 1]

repeated[:10]

#!pip3 install -U textblob

#!python3 -m textblob.download_corpora

from textblob import TextBlob

# subjectivity
def get_subjectivity(text):
  return TextBlob(text).sentiment.subjectivity
# polarity
def get_polarity(text):
  return TextBlob(text).sentiment.polarity

#insert column
df['subjectivity'] = df['clean_text'].apply(get_subjectivity)
df['polarity'] = df['text'].apply(get_polarity)

df.head()

# labelling words

def get_labels(score):
  if score < 0:
    return 'negative'
  elif score == 0:
    return 'negative'
  else:
    return 'positive'

df['get_labels'] = df['polarity'].apply(get_labels)
df.head()

from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer

tvec = TfidfVectorizer(stop_words=None, max_features=100000, ngram_range=(1, 3))
lr = LogisticRegression()

from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import precision_score, recall_score, f1_score

def lr_cv(splits, X, Y, pipeline, average_method):
    
    kfold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=777)
    accuracy = []
    precision = []
    recall = []
    f1 = []
    for train, test in kfold.split(X, Y):
        lr_fit = pipeline.fit(X[train], Y[train])
        prediction = lr_fit.predict(X[test])
        scores = lr_fit.score(X[test],Y[test])
        
        accuracy.append(scores * 100)
        precision.append(precision_score(Y[test], prediction, average=average_method)*100)
        print('              negative    neutral     positive')
        print('precision:',precision_score(Y[test], prediction, average=None))
        recall.append(recall_score(Y[test], prediction, average=average_method)*100)
        print('recall:   ',recall_score(Y[test], prediction, average=None))
        f1.append(f1_score(Y[test], prediction, average=average_method)*100)
        print('f1 score: ',f1_score(Y[test], prediction, average=None))
        print('-'*50)

    print("accuracy: %.2f%% (+/- %.2f%%)" % (np.mean(accuracy), np.std(accuracy)))
    print("precision: %.2f%% (+/- %.2f%%)" % (np.mean(precision), np.std(precision)))
    print("recall: %.2f%% (+/- %.2f%%)" % (np.mean(recall), np.std(recall)))
    print("f1 score: %.2f%% (+/- %.2f%%)" % (np.mean(f1), np.std(f1)))

from sklearn.pipeline import Pipeline

original_pipeline = Pipeline([
    ('vectorizer', tvec),
    ('classifier', lr)
])

# Train the model
lr_cv(5, df.clean_text, df.get_labels, original_pipeline, 'macro')

#import pickle

##filename = 'finalized_model.sav'
#pickle.dump(lr_cv, open(filename, 'wb'))

#import csv

#df.to_csv('cleaned_data.csv', index=False)